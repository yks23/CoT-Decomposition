import os
import json
import hydra
import torch
from vllm import LLM, SamplingParams
from typing import List, Tuple
import tqdm
import statistics
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
from reward import normalize_final_answer,extract_boxed_content
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch.nn.functional as F
from visualize import plot_by_task_samples
from batchconfig import max_token_dataset, max_batch_size, is_multi_choice, load_dataset_by_name
from model import EntropyCalculator,check_resume, load_model
# ======================
# åˆ†å¸ƒå¼åˆå§‹åŒ–
# ======================
def ddp_setup():
    init_process_group(backend="nccl")
    torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))





def generate_and_compute_entropy(
    model,
    tokenizer,
    input_text,
    max_new_tokens,
    device="cuda",
    stop_token=None,
    sample_num=1,
    temperature=1.0,
    top_p=0.7,
    need_static=True
):
    """æ”¯æŒå¤šæ¬¡é‡‡æ ·ï¼ˆé«˜æ•ˆç‰ˆï¼Œä¸€æ¬¡å‰å‘ç”Ÿæˆå¤šæ ·æœ¬ï¼‰"""
    # æ‰¹å¤„ç†è¾“å…¥
    input_ids = tokenizer(
        input_text,
        return_tensors="pt",
        padding=True,
        padding_side="left"
    ).to(device)
    B = len(input_text)  # batch size
    print(B,sample_num)
    model.eval()
    model.to(device)
    logit_processors = []

    with torch.no_grad():
        outputs = model.generate(
            **input_ids,
            max_length=input_ids.input_ids.shape[1] + max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            num_return_sequences=sample_num,
            return_dict_in_generate=True,
            output_scores=True,
            eos_token_id=[tokenizer.eos_token_id] if stop_token is None else [
                tokenizer.eos_token_id,
                tokenizer.convert_tokens_to_ids(stop_token)[0],
            ],
            logits_processor=logit_processors
        )
 
        logits = outputs.scores  # list[step](B*sample_num, vocab)
        sequences = outputs.sequences[:, input_ids.input_ids.shape[1]:]  # only new tokens

        total_size = B * sample_num
        assert sequences.shape[0] == total_size

        # åˆå§‹åŒ–æŒ‰ sample æ’åˆ—
        all_results = []

        for s in range(sample_num):
            # æ¯ä¸ª batch çš„ç¬¬ s ä¸ª sample
            batch_tokens = sequences[s::sample_num]  # (B, L)
            
            # è®¡ç®— entropy: steps x B -> B x L
            step_entropies = []
            step_confidences = []
            for step, logit in enumerate(logits):
                step_logit = logit[s::sample_num]  # (B, vocab)
                step_prob = torch.softmax(step_logit, dim=-1)
                step_entropy = -torch.sum(step_prob * torch.log(step_prob + 1e-8), dim=-1)
                
                step_confidence = torch.topk(step_prob, k=5, dim=-1).values
                step_confidence = torch.mean(torch.log(step_confidence + 1e-8), dim=-1)
                
                step_confidences.append(step_confidence.cpu())  # (B,)
                step_entropies.append(step_entropy.cpu())  # (B,)

            # è½¬ç½® step_entropies -> B x L
            batch_entropy = torch.stack(step_entropies, dim=1).tolist()  # (B, L)
            batch_confidence = torch.stack(step_confidences, dim=1).tolist()  # (B, L)
            # æ¯ä¸ª sample å¯¹åº” batch å†…æ¯ä¸ªè¾“å…¥çš„ (tokens, entropy)
            tokens= []
            entropies=[]
            confidences =[]
            for i in range(B):
                tokens.append(batch_tokens[i].cpu().tolist())
                entropies.append(batch_entropy[i])
                confidences.append(batch_confidence[i])
                

            all_results.append((tokens, entropies, confidences))
    if need_static:
        return all_results
    else:
        return [all_results[i][0] for i in range(len(all_results))]



def calculate_entropy(token_logprob_dict):
    entropy = 0.0
    for id,logprob in token_logprob_dict.items():
        prob = torch.exp(torch.tensor(logprob.logprob))
        entropy -= prob * logprob.logprob
    return entropy.item()

def calculate_confidence(token_logprob_dict,K=5):
    sorted_logprobs = sorted([logprob.logprob for id, logprob in token_logprob_dict.items()], reverse=True)
    top_k_logprobs = sorted_logprobs[:K]
    confidence = -sum(top_k_logprobs)
    return confidence

def generate_and_compute_entropy_vllm(
    model: LLM,
    input_text: List[str],
    max_new_tokens: int,
    sample_num: int = 1,
    temperature: float = 1.0,
    top_p: float = 0.7,
    stop_token: str = None
):
    """
    ä½¿ç”¨ vllm ç”Ÿæˆæ–‡æœ¬å¹¶è·å–æ¯ä¸ªç”Ÿæˆ token çš„å¯¹æ•°æ¦‚ç‡ã€‚

    Args:
        model (LLM): vLLM å®ä¾‹ã€‚
        input_text (List[str]): è¾“å…¥æç¤ºçš„åˆ—è¡¨ã€‚
        max_new_tokens (int): ç”Ÿæˆçš„æ–° token çš„æœ€å¤§æ•°é‡ã€‚
        sample_num (int): æ¯ä¸ªæç¤ºç”Ÿæˆçš„æ ·æœ¬æ•°é‡ã€‚
        temperature (float): é‡‡æ ·æ¸©åº¦ã€‚
        top_p (float): top-p é‡‡æ ·å€¼ã€‚
        stop_token (str): åœæ­¢ç”Ÿæˆçš„ tokenã€‚

    Returns:
        ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ã€‚æ¯ä¸ªå…ƒç»„åŒ…å«ä¸€ä¸ªç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨å’Œæ¯ä¸ªæ–‡æœ¬å¯¹åº”çš„ token å¯¹æ•°æ¦‚ç‡åˆ—è¡¨ã€‚
    """
    
    # é…ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        n=sample_num,
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_new_tokens,
        stop_token_ids=[model.get_tokenizer().convert_tokens_to_ids(stop_token)] if stop_token else None,
        logprobs=1  # è¯·æ±‚æ¯ä¸€æ­¥ç”Ÿæˆçš„ token çš„å¯¹æ•°æ¦‚ç‡
    )

    # ç”Ÿæˆæ–‡æœ¬å’Œå¯¹æ•°æ¦‚ç‡
    outputs = model.generate(input_text, sampling_params)

    results = []
    
    for output in outputs:
        generated_texts = []
        token_logprobs = []
        confidence_list = []
        # è·å–æ‰¹æ¬¡å†…æ¯ä¸ªæ ·æœ¬çš„ç»“æœ
        for sequence in output.outputs:
            generated_texts.append(sequence.text)
            
            # logprobs å¯¹è±¡æ˜¯å­—å…¸çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸å¯¹åº”ä¸€ä¸ª token
            logprob_list = [calculate_entropy(token) for token in sequence.logprobs]
            
            confidence_list.append([calculate_confidence(token) for token in sequence.logprobs])
            
            token_logprobs.append(logprob_list)

        results.append((generated_texts, token_logprobs, confidence_list))

    return results



def decode_predictions(tokenizer, tokens, special_tokens, answer_seg):
    pred_texts = [
        decode_with_selected_special_tokens(tokenizer, special_tokens, ids) for ids in tokens
    ]
    answers = [txt[-200:] for txt in pred_texts]
    return pred_texts, answers


def decode_with_selected_special_tokens(tokenizer, special_tokens, token_ids):
            """
            è§£ç  token åºåˆ—ï¼Œä»…ä¿ç•™æŒ‡å®šçš„ç‰¹æ®Š tokenï¼Œç§»é™¤å…¶ä»–ç‰¹æ®Š tokenã€‚

            å‚æ•°:
            - tokenizer: Tokenizer å®ä¾‹ï¼Œç”¨äºè§£ç å’Œè½¬æ¢ tokenã€‚
            - special_tokens: éœ€è¦ä¿ç•™çš„ç‰¹æ®Š token çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
            - token_ids: å¾…è§£ç çš„ token åºåˆ—ã€‚

            è¿”å›:
            - è¿‡æ»¤åçš„è§£ç æ–‡æœ¬å­—ç¬¦ä¸²ã€‚
            """
            # è·å–ä¿ç•™çš„ç‰¹æ®Š token çš„ ID
            special_tokens +=['<THINKING>','</THINKING>','<EXPLORATION>','</EXPLORATION>','<EXECUTION>','</EXECUTION>']
            keep_special_token_ids = {tokenizer.convert_tokens_to_ids(token) for token in special_tokens}
            
            # è§£ç ä¹‹å‰ï¼Œè¿‡æ»¤æ‰ä¸åœ¨ä¿ç•™åˆ—è¡¨ä¸­çš„ç‰¹æ®Š token
            filtered_token_ids = [
                token_id for token_id in token_ids
                if token_id not in tokenizer.all_special_ids or token_id in keep_special_token_ids
            ]
            
            # ä½¿ç”¨è¿‡æ»¤åçš„ token_ids è¿›è¡Œè§£ç 
            decoded_text = tokenizer.decode(filtered_token_ids, skip_special_tokens=False)
            return decoded_text

# ======================
# è¯„ä¼°å±‚
# ======================
def check_format(text, checklist):
    return all(key in text for key in checklist)

def multi_choice_evaluate(pred, gt):
    boxed_answer = pred.split("boxed{")[-1]
    boxed_answer = boxed_answer[:10]
    boxed_answer = boxed_answer.split("}")[0]
    if len(boxed_answer)==1:
        if boxed_answer==gt['answer_idx']:
            return True
    else:
        if normalize_final_answer(gt['answer']) in normalize_final_answer(pred[:100]):
            return True
    return False
    
    
def evaluate_batch(questions, gt_answers, samples, cfg, tokenizer):
    """
    samples: List[ (tokens, entropies) ]  # æ¯æ¬¡é‡‡æ ·çš„ç»“æœ
    """
    K = len(questions)
    sample_num = len(samples)

    # åˆå§‹åŒ–ç†µè®¡ç®—å™¨
    entropy_calculator = EntropyCalculator(tokenizer)
    
    # æ”¶é›†æ¯æ¬¡é‡‡æ ·çš„ decode ç»“æœ
    all_pred_texts = []
    all_answers = []
    for tokens, _,_ in samples:
        pred_texts, answers = decode_predictions(tokenizer, tokens, cfg.eval.head + cfg.eval.tail, cfg.eval.answer_seg)
        all_pred_texts.append(pred_texts)
        all_answers.append(answers)

    # è®¡ç®—ç†µç»Ÿè®¡ï¼ˆä½¿ç”¨ç¬¬å…¨éƒ¨ä¸ªæ ·æœ¬ï¼‰
    entropy_stats = {}
    if samples:
        all_tokens = []
        all_entropies=[]
        all_confidences = []
        for tokens, entropies,confidences in samples:
            all_tokens.extend(tokens)
            all_entropies.extend(entropies)
            all_confidences.extend(confidences)
        for start_token, end_token in zip(cfg.eval.head, cfg.eval.tail):
            stats = entropy_calculator.calculate_batch_entropy_stats(
                (all_tokens, all_entropies,all_confidences),start_token, end_token
            )
            key = f"{start_token}_{end_token}"
            entropy_stats[key] = stats
    # torch.save(all_entropies, 'entropies.pt')
    # é€é¢˜è¯„ä¼°
    results = []
    success_avg, success_best, format_avg = 0, 0, 0

    for i in range(K):
        per_sample_success = []
        per_sample_format = []
        per_sample_outputs = []

        for s in range(sample_num):
            pred = all_answers[s][i]
            
            tokens = samples[s][0][i]
            entropies = samples[s][1][i]
            confidences = samples[s][2][i]
            full_text = all_pred_texts[s][i]
            boxed_count = full_text.count("boxed")
            
            if isinstance(gt_answers[i],dict):
                succ = multi_choice_evaluate(pred, gt_answers[i])
                pre_answer = pred.split("boxed{")[-1]
            else:
                gt,p_float = normalize_final_answer(gt_answers[i])
                succ = False
                pre_answer = ""
                # find every boxed and compare with gt
                for pre_answer in extract_boxed_content(full_text):
                    pre_answer, q_float = normalize_final_answer(pre_answer)
                    if pre_answer == gt or (q_float is not None and q_float == p_float):
                        succ = True
                        break
            fmt = check_format(full_text, cfg.eval.checklist)
            per_sample_success.append(succ)
            per_sample_format.append(fmt)
            
            per_sample_outputs.append({
                "answer": pre_answer,
                "full_text": full_text,
                "success": succ,
                "format_success": fmt,
                "static": entropy_calculator.calculate_entropy_stats((tokens, entropies,confidences),cfg.eval.head[0], cfg.eval.tail[0]) if cfg.eval.need_static else None,
                "boxed_count": boxed_count
            })

        # å¹³å‡æˆåŠŸç‡ï¼šæ ·æœ¬æˆåŠŸç‡çš„å‡å€¼
        success_avg += sum(per_sample_success) / sample_num
        format_avg += sum(per_sample_format) / sample_num
        # æœ€å¥½æˆåŠŸç‡ï¼šåªè¦æœ‰ä¸€æ¬¡æˆåŠŸå°±ç®—æˆåŠŸ
        success_best += 1 if any(per_sample_success) else 0

        results.append({
            "question": questions[i],
            "gt_answer": gt_answers[i],
            "samples": per_sample_outputs,
            "avg_success": sum(per_sample_success) / sample_num,
            "best_success": 1 if any(per_sample_success) else 0,
            "finish_rate": sum([1 for out in per_sample_outputs if out['boxed_count']>0]) / sample_num,
            "finish_and_correct_rate": sum([1 for out in per_sample_outputs if out['boxed_count']>0 and out['success']]) / sample_num
        })

    batch_stats = {
        "avg_success": success_avg / K,
        "best_success": success_best / K,
        "format_avg": format_avg / K,
        "all_number": K,
        
        "entropy_stats": entropy_stats  # æ·»åŠ ç†µç»Ÿè®¡
    }
    
    return results, batch_stats


# ======================
# æ—¥å¿—å±‚
# ======================
def log_batch_stats(batch_idx, batch_stats, rank):
    if rank != 0:
        return
        
    avg_rate = batch_stats["avg_success"]
    best_rate = batch_stats["best_success"]
    fmt_rate = batch_stats["format_avg"]
    
    print(f"\n[Batch {batch_idx}] ğŸ² å¹³å‡æˆåŠŸç‡: {avg_rate:.2%} | æœ€å¥½æˆåŠŸç‡: {best_rate:.2%} | æ ¼å¼ç‡: {fmt_rate:.2%}")
    
    # æ‰“å°ç†µç»Ÿè®¡ä¿¡æ¯
    if "entropy_stats" in batch_stats:
        print("ğŸ“Š ç†µç»Ÿè®¡:")
        for key, stats in batch_stats["entropy_stats"].items():
            if stats["sample_count"] > 0:
                print(f"   {key}: é•¿åº¦={stats['avg_length']:.1f}, "
                      f"æ€»ç†µ={stats['avg_total_entropy']:.2f}, "
                      f"å¹³å‡ç†µ={stats['avg_entropy_per_token']:.4f} "
                      f"({stats['sample_count']}æ ·æœ¬)")


# ======================
# åˆ†å¸ƒå¼æ•°æ®åŠ è½½
# ======================
def get_distributed_dataloader(dataset, batch_size, rank, world_size):
    sampler = DistributedSampler(
        dataset, 
        num_replicas=world_size, 
        rank=rank, 
        shuffle=False,
        drop_last=False
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    indices = list(sampler)
    return [dataset[i] for i in indices], len(indices)


# ======================
# ç»“æœåˆå¹¶
# ======================

def merge_results_from_all_ranks(save_path, seed, world_size,save_head=False):
    if torch.distributed.get_rank() != 0:
        return
        
    all_results = []
    all_static = {"avg_success": 0, "best_success": 0, "format_avg": 0, "all_number": 0, "entropy_stats": {}}
    total_samples = 0
    finish_count = 0
    finish_and_correct_count = 0
    for rank in range(world_size):
        result_file = os.path.join(save_path, f"result_{seed}_rank{rank}.json")
        static_file = os.path.join(save_path, f"static_{seed}_rank{rank}.json")
        
        if os.path.exists(result_file) and os.path.exists(static_file):
            with open(result_file, "r") as f:
                results = json.load(f)
            with open(static_file, "r") as f:
                static = json.load(f)
            
            all_results.extend(results)
            
            # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
            if static["all_number"] > 0:
                weight = static["all_number"]
                all_static["avg_success"] = (all_static["avg_success"] * total_samples + static["avg_success"] * weight) / (total_samples + weight)
                all_static["best_success"] = (all_static["best_success"] * total_samples + static["best_success"] * weight) / (total_samples + weight)
                all_static["format_avg"] = (all_static["format_avg"] * total_samples + static["format_avg"] * weight) / (total_samples + weight)
                total_samples += weight
                
                # åˆå¹¶ç†µç»Ÿè®¡
                for key, stats in static.get("entropy_stats", {}).items():
                    if key not in all_static["entropy_stats"]:
                        all_static["entropy_stats"][key] = stats.copy()
                    else:
                        old_stats = all_static["entropy_stats"][key]
                        total_sample_count = old_stats["sample_count"] + stats["sample_count"]
                        
                        if total_sample_count > 0:
                            all_static["entropy_stats"][key] = {
                                "avg_length": (old_stats["avg_length"] * old_stats["sample_count"] + stats["avg_length"] * stats["sample_count"]) / total_sample_count,
                                "avg_total_entropy": (old_stats["avg_total_entropy"] * old_stats["sample_count"] + stats["avg_total_entropy"] * stats["sample_count"]) / total_sample_count,
                                "avg_entropy_per_token": (old_stats["avg_entropy_per_token"] * old_stats["sample_count"] + stats["avg_entropy_per_token"] * stats["sample_count"]) / total_sample_count,
                                "sample_count": total_sample_count
                            }
    entropies=[]
    for result in all_results:
        avg_success = result.get("avg_success", 0)
        success_entropy = -avg_success*torch.log(torch.tensor(avg_success) + 1e-8) - (1 - avg_success)*torch.log(torch.tensor(1 - avg_success) + 1e-8)
        entropies.append(success_entropy.item())
        finish_count += sum([1 for out in result['samples'] if out['boxed_count']>0])
        finish_and_correct_count += sum([1 for out in result['samples'] if out['boxed_count']>0 and out['success']])
    all_static['finish_rate'] = finish_count/ (len(all_results)*len(all_results[0]['samples'])) if total_samples>0 else 0
    all_static['finish_and_correct_rate'] = finish_and_correct_count/ finish_count if finish_count>0 else 0
    all_static['success_entropy'] = sum(entropies)/len(entropies) if len(entropies) > 0 else 0
    
    if save_head:
        q2a = {}
        for result in all_results:
            question = result['question']
            answer = result['samples'][0]['full_text']
            q2a[question] = answer
        with open(os.path.join(save_path, f"q2a.json"), "w") as f:
            json.dump(q2a, f, ensure_ascii=False, indent=4)
    
    # ä¿å­˜åˆå¹¶åçš„ç»“æœ
    if all_results:
        with open(os.path.join(save_path, f"result_{seed}_merged.json"), "w") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=4)
        with open(os.path.join(save_path, f"static_{seed}_merged.json"), "w") as f:
            json.dump(all_static, f, ensure_ascii=False, indent=4)
        
        print(f"\nâœ… åˆå¹¶å®Œæˆ: æ€»å…± {len(all_results)} ä¸ªæ ·æœ¬")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: å¹³å‡æˆåŠŸç‡={all_static['avg_success']:.2%}, "
              f"æœ€å¥½æˆåŠŸç‡={all_static['best_success']:.2%}, "
              f"æ ¼å¼ç‡={all_static['format_avg']:.2%}")
    
    
    
    plot_by_task_samples(all_results, save_path,10)


# ======================
# ä¸»æ§
# ======================
@hydra.main(version_base=None, config_path="config", config_name="eval")
def eval_main(cfg):
    ddp_setup()
    rank = torch.distributed.get_rank()
    world_size = torch.distributed.get_world_size()
    device = f"cuda:{rank}"
    torch.manual_seed(cfg.eval.seed + rank)  # ä¸åŒrankä½¿ç”¨ä¸åŒçš„éšæœºç§å­
    model, tokenizer = load_model(cfg.model, device)
        
        # ä½¿ç”¨DDPåŒ…è£…æ¨¡å‹
    model = DDP(model, device_ids=[rank])
    
    overall_stats = {}
    
    if 'all' in cfg.eval.dataset:
        cfg.eval.dataset = ["gsm8k", "math", "aime24", "aime25", "amc23", "math500", "minerva", "olympiad_bench"]
    if 'med' in cfg.eval.dataset:
        cfg.eval.dataset = ["medqa","medmcqa","pubmedqa","clinical_knowledge","college_biology","college_medicine","medical_genetics","professional_medicine","anatomy"]
    for dataset_name in cfg.eval.dataset:
        dataset, dataset_name = load_dataset_by_name(dataset_name)
        if cfg.eval.batch_size == -1:
            batch_size = max_batch_size.get(dataset_name, 10) // cfg.eval.sample_num  # æ¯ä¸ªrankçš„batch size
        else:
            batch_size = cfg.eval.batch_size // cfg.eval.sample_num  # æ¯ä¸ªrankçš„batch size
        if cfg.eval.max_new_tokens == -1:
            max_tokens = max_token_dataset.get(dataset_name, 1000)
        else:
            max_tokens = cfg.eval.max_new_tokens    
        
        save_path = os.path.join(cfg.eval.save_path, dataset_name)
        os.makedirs(save_path, exist_ok=True)
        if cfg.eval.get('resume', False):
            results, static = check_resume(save_path, cfg.eval.seed, rank)
            print(f"[Rank {rank}] Resuming evaluation for dataset {dataset_name} from {len(results)} existing results.")
        else:
            results, static = [], {}
        
        subset, subset_size = get_distributed_dataloader(dataset, batch_size, rank, world_size)
        # åˆå§‹åŒ– tqdmï¼ˆåªåœ¨ rank0ï¼‰
        if rank == 0:
            bar = tqdm.tqdm(total=len(dataset), desc=f"Processing {dataset_name}", position=0, leave=True)
        else:
            bar = None

        # --- æœ¬åœ°ç´¯è®¡é‡ï¼ˆç”¨äº all_reduce åŒæ­¥å…¨å±€ç´¯è®¡ï¼‰ ---
        # local_cum_processed: æœ¬ rank ç´¯è®¡å·²å¤„ç†æ ·æœ¬æ•°ï¼ˆresume æ—¶ä» static è¯»ï¼‰
        if static and "all_number" in static and static["all_number"] > 0:
            local_cum_processed = int(static["all_number"])
            # ä¸ºäº†èƒ½æ­£ç¡®åˆå¹¶ avg_success/best_success/formatï¼Œæˆ‘ä»¬ç”¨ sum è€Œä¸æ˜¯å¹³å‡å€¼
            local_avg_succ_sum = float(static.get("avg_success", 0.0)) * local_cum_processed
            local_best_succ_sum = float(static.get("best_success", 0.0)) * local_cum_processed
            local_fmt_sum = float(static.get("format_avg", 0.0)) * local_cum_processed
            # results å·²åŒ…å«æœ¬ rank çš„å†å²ç»“æœï¼ˆç”± check_resume æä¾›ï¼‰
        else:
            local_cum_processed = 0
            local_avg_succ_sum = 0.0
            local_best_succ_sum = 0.0
            local_fmt_sum = 0.0

        # å¦‚æœæœ‰å·²å¤„ç†çš„æœ¬åœ°ç»“æœï¼Œéœ€è¦æŠŠå…¨å±€åˆå§‹è¿›åº¦åŒæ­¥åˆ° rank0ï¼ˆé¿å…ç›´æ¥ç”¨ bar.update(len(results))ï¼‰
        init_buf = torch.tensor(
            [local_cum_processed, local_avg_succ_sum, local_best_succ_sum, local_fmt_sum],
            device=device, dtype=torch.float64
        )
        if cfg.eval.backend=='hf':
            torch.distributed.all_reduce(init_buf, op=torch.distributed.ReduceOp.SUM)
        if rank == 0:
            global_processed_init = int(init_buf[0].item())
            if global_processed_init > 0:
                # è®¾ç½® bar çš„åˆå§‹ä½ç½®ï¼ˆç»å¯¹è®¾å®šï¼Œé¿å… update å¯¼è‡´è´Ÿæ•°ï¼‰
                bar.n = global_processed_init
                bar.refresh()
                # æ‰“å°åˆå§‹å…¨å±€ç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰
                avg_success_init = init_buf[1].item() / global_processed_init
                best_success_init = init_buf[2].item() / global_processed_init
                fmt_init = init_buf[3].item() / global_processed_init
                print(f"[Init Global] processed={global_processed_init} | "
                      f"avg_success={avg_success_init:.2%} | best_success={best_success_init:.2%} | format={fmt_init:.2%}")
        # å¦‚æœå·²ç»æœ‰ resultsï¼ˆæœ¬ rank resumeï¼‰ï¼Œè·³è¿‡å·²å¤„ç†çš„ç´¢å¼•
        start_idx = len(results)
        # å¦‚æœ bar éœ€è¦åæ˜ å·²ç»å†™å…¥æ–‡ä»¶çš„æœ¬åœ°ç»“æœæ•°é‡ï¼Œä¹Ÿç”±ä¸Šé¢åŒæ­¥ç¡®ä¿ä¸€è‡´
        for i in range(start_idx, subset_size, batch_size):
            batch_data = subset[i:i + batch_size]
            guidelines = [d.get('exploration', '') for d in batch_data]

            questions = [
                tokenizer.apply_chat_template(
                    [{"role": "user", "content": cfg.eval.get('question_prefix','')+q["question"] + cfg.eval.get('question_suffix','').replace("<guideline>", g)}],
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=cfg.eval.get("enable_thinking", True)
                ) + cfg.eval.get("solution_prefix", "").replace("<guideline>", g)
                for q, g in zip(batch_data, guidelines)
            ]
            
            if cfg.eval.get("head_path", None) is not None:
                with open(cfg.eval.head_path, "r") as f:
                    head_dict = json.load(f)
                questions = [q+head_dict.get(q, questions[idx]) for idx, q in enumerate(questions)]
            gt_answers = [d["answer"] for d in batch_data]

            # === å¤šæ¬¡é‡‡æ · ===
            samples = generate_and_compute_entropy(
                model.module, tokenizer, questions,  # ä½¿ç”¨model.moduleè®¿é—®åŸå§‹æ¨¡å‹
                max_tokens,
                device=device,
                stop_token=cfg.eval.get("stop_token", None),
                sample_num=cfg.eval.sample_num,
                temperature=cfg.eval.temperature,
                top_p=cfg.eval.top_p,
            )

            batch_results, batch_stats = evaluate_batch(questions, gt_answers, samples, cfg, tokenizer)
            results.extend(batch_results)

            # æ›´æ–°æœ¬åœ° staticï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
            if not static:
                static = {"avg_success": 0, "best_success": 0, "format_avg": 0, "all_number": 0, "entropy_stats": {}}
            
            # æ›´æ–°å‡†ç¡®ç‡ç»Ÿè®¡ï¼ˆæŒ‰ä½ åŸæ¥åŠ æƒå¹³å‡é€»è¾‘ï¼‰
            static["avg_success"] = (static["avg_success"] * static["all_number"] + batch_stats["avg_success"] * batch_stats["all_number"]) / (static["all_number"] + batch_stats["all_number"])
            static["best_success"] = (static["best_success"] * static["all_number"] + batch_stats["best_success"] * batch_stats["all_number"]) / (static["all_number"] + batch_stats["all_number"])
            static["format_avg"] = (static["format_avg"] * static["all_number"] + batch_stats["format_avg"] * batch_stats["all_number"]) / (static["all_number"] + batch_stats["all_number"])
            static["all_number"] += batch_stats["all_number"]
            
            # æ›´æ–°ç†µç»Ÿè®¡ï¼ˆåŠ æƒå¹³å‡ï¼‰
            if "entropy_stats" in batch_stats:
                for key, new_stats in batch_stats["entropy_stats"].items():
                    if key not in static["entropy_stats"]:
                        static["entropy_stats"][key] = new_stats.copy()
                    else:
                        old_stats = static["entropy_stats"][key]
                        total_samples = old_stats["sample_count"] + new_stats["sample_count"]
                        if total_samples > 0:
                            static["entropy_stats"][key] = {
                                "avg_length": (old_stats["avg_length"] * old_stats["sample_count"] + new_stats["avg_length"] * new_stats["sample_count"]) / total_samples,
                                "avg_total_entropy": (old_stats["avg_total_entropy"] * old_stats["sample_count"] + new_stats["avg_total_entropy"] * new_stats["sample_count"]) / total_samples,
                                "avg_entropy_per_token": (old_stats["avg_entropy_per_token"] * old_stats["sample_count"] + new_stats["avg_entropy_per_token"] * new_stats["sample_count"]) / total_samples,
                                "sample_count": total_samples
                            }

            # ä¿å­˜å½“å‰rankçš„ç»“æœ
            with open(os.path.join(save_path, f"result_{cfg.eval.seed}_rank{rank}.json"), "w") as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            with open(os.path.join(save_path, f"static_{cfg.eval.seed}_rank{rank}.json"), "w") as f:
                json.dump(static, f, ensure_ascii=False, indent=4)

            # ========== å…¨å±€åŒæ­¥ï¼ˆç´¯è®¡é‡ï¼‰ ==========
            # æŠŠæœ¬ rank çš„ç´¯è®¡å€¼ç´¯åŠ ï¼ˆæ³¨æ„ static["all_number"] æ˜¯æœ¬ rank åˆ°ç›®å‰ä¸ºæ­¢çš„ç´¯è®¡ï¼‰
            local_cum_processed = int(static["all_number"])
            # local çš„ç´¯è®¡å’Œï¼ˆç”¨äºç®—æ€»ä½“å¹³å‡ï¼‰
            local_avg_succ_sum += batch_stats["avg_success"] * batch_stats["all_number"]
            local_best_succ_sum += batch_stats["best_success"] * batch_stats["all_number"]
            local_fmt_sum += batch_stats["format_avg"] * batch_stats["all_number"]

            buf = torch.tensor(
                [local_cum_processed, local_avg_succ_sum, local_best_succ_sum, local_fmt_sum],
                device=device, dtype=torch.float64
            )
            torch.distributed.all_reduce(buf, op=torch.distributed.ReduceOp.SUM)

            if rank == 0:
                global_processed = int(buf[0].item())
                if global_processed > 0:
                    avg_success = buf[1].item() / global_processed
                    best_success = buf[2].item() / global_processed
                    format_rate = buf[3].item() / global_processed

                    # ç›´æ¥æŠŠ bar è®¾ä¸ºç»å¯¹å€¼å¹¶åˆ·æ–°ï¼ˆé¿å…è´Ÿæ•°ï¼‰
                    if bar is not None:
                        bar.n = global_processed
                        bar.refresh()

                    # æ‰“å°å…¨å±€å®æ—¶ç»Ÿè®¡
                    print(f"[Global] âœ… processed={global_processed} | "
                          f"avg_success={avg_success:.2%} | best_success={best_success:.2%} | "
                          f"format={format_rate:.2%}")

            # æœ¬åœ°æ—¥å¿—ï¼ˆæŒ‰ä½ åŸæ¥é€»è¾‘ï¼‰
            log_batch_stats(i // batch_size, batch_stats, rank)
            if rank == 0:
                print("ç´¯è®¡ç»Ÿè®¡:")
                log_batch_stats(i // batch_size, static, rank)

        overall_stats[dataset_name] = static
        
        torch.distributed.barrier()
        
        # åˆå¹¶æ‰€æœ‰rankçš„ç»“æœ
        merge_results_from_all_ranks(save_path, cfg.eval.seed, world_size,cfg.eval.get('save_head', False))
        
        if bar is not None:
            bar.close()

    # ä¿å­˜æ•´ä½“ç»Ÿè®¡
    if rank == 0:
        with open(os.path.join(cfg.eval.save_path, f"overall_static_{cfg.eval.seed}.json"), "w") as f:
            json.dump(overall_stats, f, ensure_ascii=False, indent=4)

    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    destroy_process_group()

if __name__ == "__main__":
    eval_main()
    

"""
usage:
torchrun --nproc_per_node=8 --nnodes=1 --node_rank=0
torchrun --nproc_per_node=8 --master-port 20000 eval.py --config-path config --config-name test

"""